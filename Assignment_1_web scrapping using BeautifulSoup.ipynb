{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eeab414",
   "metadata": {},
   "source": [
    "# Web Scrapping Using BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b374231c",
   "metadata": {},
   "source": [
    " Importing all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bde727db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING REQUESTS TO GET THE SOURCE CODE OF THE FILE\n",
    "# IMPORTING bs4 FOR BEAUTIFUL SOUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e2786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40df04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing BeautifulSoup from bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7633e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919095c8",
   "metadata": {},
   "source": [
    "# Question-1: Write a python program to display all the header tags from wikipedia.org "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3325a1db",
   "metadata": {},
   "source": [
    "To get the source code of the webpage using requests.get\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "304bea5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page1 = requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
    "page1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0094ee84",
   "metadata": {},
   "source": [
    "To get the page content using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "bdc6f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1 = BeautifulSoup(page1.content)\n",
    "#soup1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4593a0",
   "metadata": {},
   "source": [
    "Scrapping all the header tags from the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "fb4ea4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\"><span class=\"mw-page-title-main\">Main Page</span></h1>"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrapping h1 tag using find method\n",
    "header_1 = soup1.find('h1')\n",
    "header_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "ae61781e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From today's featured article\""
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrapping h2 tag\n",
    "header_2 = soup1.find('h2')\n",
    "header_2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "7b724426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Personal tools'"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrapping h3 tag's title uing text\n",
    "header_3 = soup1.find('h3')\n",
    "header_3.text.split(\"\\n\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "9caeea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the header tags from wikipedia.org are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Main Page'],\n",
       " ['Welcome to Wikipedia'],\n",
       " [\"From today's featured article\"],\n",
       " ['Did you know\\xa0...'],\n",
       " ['In the news'],\n",
       " ['On this day'],\n",
       " [\"Today's featured picture\"],\n",
       " ['Other areas of Wikipedia'],\n",
       " [\"Wikipedia's sister projects\"],\n",
       " ['Wikipedia languages'],\n",
       " ['Navigation menu'],\n",
       " ['', 'Personal tools', ''],\n",
       " ['', 'Namespaces', ''],\n",
       " ['', 'Views', ''],\n",
       " ['', 'Navigation', ''],\n",
       " ['', 'Contribute', ''],\n",
       " ['', 'Tools', ''],\n",
       " ['', 'Print/export', ''],\n",
       " ['', 'In other projects', ''],\n",
       " ['', 'Languages', '']]"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scrapping all the header tags using find_all in the list \n",
    "headers=[] # empty list \n",
    "\n",
    "for i in soup1.find_all(['h1','h2','h3','h4','h5','h6']):\n",
    "    headers.append(i.text.split(\"\\n\"))\n",
    "\n",
    "print(\"All the header tags from wikipedia.org are:\")\n",
    "\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b197027b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7ef622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60218784",
   "metadata": {},
   "source": [
    "# Question 2: Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2072aed",
   "metadata": {},
   "source": [
    " To get the source code of the webpage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "6f09bb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page2 = requests.get('https://www.imdb.com/list/ls055592025/?sort=user_rating,desc&st_dt=&mode=detail&page=1')\n",
    "page2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309aa2a6",
   "metadata": {},
   "source": [
    "To get the page content using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "2e6d642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2 = BeautifulSoup(page2.content)\n",
    "#soup2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce6cb6f",
   "metadata": {},
   "source": [
    "SCRAPPING THE NAME AND YEAR OF RELEASE OF 100 MOVIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "e3ccbce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shawshank Redemption   (1994)\n",
      "The Godfather   (1972)\n",
      "The Godfather Part II   (1974)\n",
      "Schindler's List   (1993)\n",
      "The Lord of the Rings: The Return of the King   (2003)\n",
      "12 Angry Men   (1957)\n",
      "Pulp Fiction   (1994)\n",
      "Forrest Gump   (1994)\n",
      "Il buono, il brutto, il cattivo   (1966)\n",
      "Goodfellas   (1990)\n",
      "One Flew Over the Cuckoo's Nest   (1975)\n",
      "It's a Wonderful Life   (1946)\n",
      "The Green Mile   (1999)\n",
      "The Silence of the Lambs   (1991)\n",
      "Saving Private Ryan   (1998)\n",
      "Star Wars   (1977)\n",
      "Gladiator   (2000)\n",
      "Apocalypse Now   (1979)\n",
      "Casablanca   (1942)\n",
      "The Pianist   (2002)\n",
      "Psycho   (1960)\n",
      "Rear Window   (1954)\n",
      "City Lights   (1931)\n",
      "Raiders of the Lost Ark   (1981)\n",
      "Braveheart   (1995)\n",
      "Amadeus   (1984)\n",
      "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb   (1964)\n",
      "Sunset Blvd.   (1950)\n",
      "The Great Dictator   (1940)\n",
      "Good Will Hunting   (1997)\n",
      "A Clockwork Orange   (1971)\n",
      "North by Northwest   (1959)\n",
      "2001: A Space Odyssey   (1968)\n",
      "Lawrence of Arabia   (1962)\n",
      "To Kill a Mockingbird   (1962)\n",
      "Citizen Kane   (1941)\n",
      "The Apartment   (1960)\n",
      "Vertigo   (1958)\n",
      "Singin' in the Rain   (1952)\n",
      "Double Indemnity   (1944)\n",
      "Taxi Driver   (1976)\n",
      "Jurassic Park   (1993)\n",
      "Gone with the Wind   (1939)\n",
      "Some Like It Hot   (1959)\n",
      "Unforgiven   (1992)\n",
      "Raging Bull   (1980)\n",
      "Chinatown   (1974)\n",
      "The Bridge on the River Kwai   (1957)\n",
      "The Treasure of the Sierra Madre   (1948)\n",
      "The Wizard of Oz   (1939)\n",
      "All Quiet on the Western Front   (1930)\n",
      "Rocky   (1976)\n",
      "The Exorcist   (1973)\n",
      "The Sound of Music   (1965)\n",
      "Jaws   (1975)\n",
      "Fargo   (1996)\n",
      "Platoon   (1986)\n",
      "The Deer Hunter   (1978)\n",
      "Ben-Hur   (1959)\n",
      "Network   (1976)\n",
      "The Best Years of Our Lives   (1946)\n",
      "On the Waterfront   (1954)\n",
      "The Third Man   (1949)\n",
      "It Happened One Night   (1934)\n",
      "The Grapes of Wrath   (1940)\n",
      "Mr. Smith Goes to Washington   (1939)\n",
      "Dances with Wolves   (1990)\n",
      "The Graduate   (1967)\n",
      "Rain Man   (1988)\n",
      "Butch Cassidy and the Sundance Kid   (1969)\n",
      "Annie Hall   (1977)\n",
      "The Maltese Falcon   (1941)\n",
      "High Noon   (1952)\n",
      "Titanic   (1997)\n",
      "Doctor Zhivago   (1965)\n",
      "E.T. the Extra-Terrestrial   (1982)\n",
      "The Searchers   (1956)\n",
      "A Streetcar Named Desire   (1951)\n",
      "Patton   (1970)\n",
      "The Philadelphia Story   (1940)\n",
      "Midnight Cowboy   (1969)\n",
      "My Fair Lady   (1964)\n",
      "Stagecoach   (1939)\n",
      "Bonnie and Clyde   (1967)\n",
      "The French Connection   (1971)\n",
      "A Place in the Sun   (1951)\n",
      "Nashville   (1975)\n",
      "The African Queen   (1951)\n",
      "Mutiny on the Bounty   (1935)\n",
      "Giant   (1956)\n",
      "Close Encounters of the Third Kind   (1977)\n",
      "West Side Story   (1961)\n",
      "Rebel Without a Cause   (1955)\n",
      "From Here to Eternity   (1953)\n",
      "Shane   (1953)\n",
      "Yankee Doodle Dandy   (1942)\n",
      "Wuthering Heights   (1939)\n",
      "American Graffiti   (1973)\n",
      "Terms of Endearment   (1983)\n",
      "An American in Paris   (1951)\n"
     ]
    }
   ],
   "source": [
    "names=[] # EMPTY LIST FOR NAMES OF THE MOVIES\n",
    "\n",
    "release_yr=[]  # EMPTY LIST FOR THE RELEASE YEAR OF THE MOVIES\n",
    "\n",
    "for i in soup2.find_all('h3',class_= \"lister-item-header\" ):\n",
    "    names.append(i.text.split(\"\\n\")[2])\n",
    "    release_yr.append(i.text.split(\"\\n\")[3])\n",
    "    \n",
    "for j in range(100):\n",
    "    print(names[j],\" \",release_yr[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca223a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1329155",
   "metadata": {},
   "source": [
    "SCRAPPING THE RATING OF THE MOVIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "887bca1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9.3', '9.2', '9', '9', '9', '9', '8.9', '8.8', '8.8', '8.7', '8.7', '8.6', '8.6', '8.6', '8.6', '8.6', '8.5', '8.5', '8.5', '8.5', '8.5', '8.5', '8.5', '8.4', '8.4', '8.4', '8.4', '8.4', '8.4', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.3', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8', '8', '8', '8', '8', '8', '8', '7.9', '7.9', '7.9', '7.9', '7.9', '7.9', '7.9', '7.8', '7.8', '7.8', '7.7', '7.7', '7.7', '7.7', '7.7', '7.7', '7.6', '7.6', '7.6', '7.6', '7.6', '7.6', '7.6', '7.5', '7.4', '7.4', '7.2'] "
     ]
    }
   ],
   "source": [
    "rating=[] # empty list for storing the ratings of the movies \n",
    "\n",
    "for i in soup2.find_all('div', class_= \"ipl-rating-star small\"):\n",
    "    rating.append(i.text.split(\"\\n\")[8])\n",
    "    \n",
    "print(rating, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "0eb715ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shawshank Redemption   (1994)   9.3\n",
      "The Godfather   (1972)   9.2\n",
      "The Godfather Part II   (1974)   9\n",
      "Schindler's List   (1993)   9\n",
      "The Lord of the Rings: The Return of the King   (2003)   9\n",
      "12 Angry Men   (1957)   9\n",
      "Pulp Fiction   (1994)   8.9\n",
      "Forrest Gump   (1994)   8.8\n",
      "Il buono, il brutto, il cattivo   (1966)   8.8\n",
      "Goodfellas   (1990)   8.7\n",
      "One Flew Over the Cuckoo's Nest   (1975)   8.7\n",
      "It's a Wonderful Life   (1946)   8.6\n",
      "The Green Mile   (1999)   8.6\n",
      "The Silence of the Lambs   (1991)   8.6\n",
      "Saving Private Ryan   (1998)   8.6\n",
      "Star Wars   (1977)   8.6\n",
      "Gladiator   (2000)   8.5\n",
      "Apocalypse Now   (1979)   8.5\n",
      "Casablanca   (1942)   8.5\n",
      "The Pianist   (2002)   8.5\n",
      "Psycho   (1960)   8.5\n",
      "Rear Window   (1954)   8.5\n",
      "City Lights   (1931)   8.5\n",
      "Raiders of the Lost Ark   (1981)   8.4\n",
      "Braveheart   (1995)   8.4\n",
      "Amadeus   (1984)   8.4\n",
      "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb   (1964)   8.4\n",
      "Sunset Blvd.   (1950)   8.4\n",
      "The Great Dictator   (1940)   8.4\n",
      "Good Will Hunting   (1997)   8.3\n",
      "A Clockwork Orange   (1971)   8.3\n",
      "North by Northwest   (1959)   8.3\n",
      "2001: A Space Odyssey   (1968)   8.3\n",
      "Lawrence of Arabia   (1962)   8.3\n",
      "To Kill a Mockingbird   (1962)   8.3\n",
      "Citizen Kane   (1941)   8.3\n",
      "The Apartment   (1960)   8.3\n",
      "Vertigo   (1958)   8.3\n",
      "Singin' in the Rain   (1952)   8.3\n",
      "Double Indemnity   (1944)   8.3\n",
      "Taxi Driver   (1976)   8.2\n",
      "Jurassic Park   (1993)   8.2\n",
      "Gone with the Wind   (1939)   8.2\n",
      "Some Like It Hot   (1959)   8.2\n",
      "Unforgiven   (1992)   8.2\n",
      "Raging Bull   (1980)   8.2\n",
      "Chinatown   (1974)   8.2\n",
      "The Bridge on the River Kwai   (1957)   8.2\n",
      "The Treasure of the Sierra Madre   (1948)   8.2\n",
      "The Wizard of Oz   (1939)   8.1\n",
      "All Quiet on the Western Front   (1930)   8.1\n",
      "Rocky   (1976)   8.1\n",
      "The Exorcist   (1973)   8.1\n",
      "The Sound of Music   (1965)   8.1\n",
      "Jaws   (1975)   8.1\n",
      "Fargo   (1996)   8.1\n",
      "Platoon   (1986)   8.1\n",
      "The Deer Hunter   (1978)   8.1\n",
      "Ben-Hur   (1959)   8.1\n",
      "Network   (1976)   8.1\n",
      "The Best Years of Our Lives   (1946)   8.1\n",
      "On the Waterfront   (1954)   8.1\n",
      "The Third Man   (1949)   8.1\n",
      "It Happened One Night   (1934)   8.1\n",
      "The Grapes of Wrath   (1940)   8.1\n",
      "Mr. Smith Goes to Washington   (1939)   8.1\n",
      "Dances with Wolves   (1990)   8\n",
      "The Graduate   (1967)   8\n",
      "Rain Man   (1988)   8\n",
      "Butch Cassidy and the Sundance Kid   (1969)   8\n",
      "Annie Hall   (1977)   8\n",
      "The Maltese Falcon   (1941)   8\n",
      "High Noon   (1952)   8\n",
      "Titanic   (1997)   7.9\n",
      "Doctor Zhivago   (1965)   7.9\n",
      "E.T. the Extra-Terrestrial   (1982)   7.9\n",
      "The Searchers   (1956)   7.9\n",
      "A Streetcar Named Desire   (1951)   7.9\n",
      "Patton   (1970)   7.9\n",
      "The Philadelphia Story   (1940)   7.9\n",
      "Midnight Cowboy   (1969)   7.8\n",
      "My Fair Lady   (1964)   7.8\n",
      "Stagecoach   (1939)   7.8\n",
      "Bonnie and Clyde   (1967)   7.7\n",
      "The French Connection   (1971)   7.7\n",
      "A Place in the Sun   (1951)   7.7\n",
      "Nashville   (1975)   7.7\n",
      "The African Queen   (1951)   7.7\n",
      "Mutiny on the Bounty   (1935)   7.7\n",
      "Giant   (1956)   7.6\n",
      "Close Encounters of the Third Kind   (1977)   7.6\n",
      "West Side Story   (1961)   7.6\n",
      "Rebel Without a Cause   (1955)   7.6\n",
      "From Here to Eternity   (1953)   7.6\n",
      "Shane   (1953)   7.6\n",
      "Yankee Doodle Dandy   (1942)   7.6\n",
      "Wuthering Heights   (1939)   7.5\n",
      "American Graffiti   (1973)   7.4\n",
      "Terms of Endearment   (1983)   7.4\n",
      "An American in Paris   (1951)   7.2\n"
     ]
    }
   ],
   "source": [
    "for j in range(100):\n",
    "    print(names[j],\" \", release_yr[j], \" \", rating[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5d360f",
   "metadata": {},
   "source": [
    "TO MAKE DATAFRAME OF ALL THE INFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8bd68df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "771371de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year of Release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>9.3</td>\n",
       "      <td>(1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>9.2</td>\n",
       "      <td>(1972)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>9</td>\n",
       "      <td>(1974)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>9</td>\n",
       "      <td>(1993)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Lord of the Rings: The Return of the King</td>\n",
       "      <td>9</td>\n",
       "      <td>(2003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Yankee Doodle Dandy</td>\n",
       "      <td>7.6</td>\n",
       "      <td>(1942)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Wuthering Heights</td>\n",
       "      <td>7.5</td>\n",
       "      <td>(1939)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>American Graffiti</td>\n",
       "      <td>7.4</td>\n",
       "      <td>(1973)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Terms of Endearment</td>\n",
       "      <td>7.4</td>\n",
       "      <td>(1983)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>An American in Paris</td>\n",
       "      <td>7.2</td>\n",
       "      <td>(1951)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Name Rating Year of Release\n",
       "0                        The Shawshank Redemption    9.3          (1994)\n",
       "1                                   The Godfather    9.2          (1972)\n",
       "2                           The Godfather Part II      9          (1974)\n",
       "3                                Schindler's List      9          (1993)\n",
       "4   The Lord of the Rings: The Return of the King      9          (2003)\n",
       "..                                            ...    ...             ...\n",
       "95                            Yankee Doodle Dandy    7.6          (1942)\n",
       "96                              Wuthering Heights    7.5          (1939)\n",
       "97                              American Graffiti    7.4          (1973)\n",
       "98                            Terms of Endearment    7.4          (1983)\n",
       "99                           An American in Paris    7.2          (1951)\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making dataframe using pandas library\n",
    "\n",
    "df = pd.DataFrame( {\"Name\": names, \"Rating\": rating, \"Year of Release\": release_yr} )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "87bff7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "# printing the length of the data \n",
    "print(len(names),len(release_yr),len(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d536d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importinf the dataframe in a csv file without index\n",
    "df.to_csv('imdb_data.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86991c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761ee5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29c7b9b5",
   "metadata": {},
   "source": [
    "# Question-3:Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d529c6",
   "metadata": {},
   "source": [
    "To get the source code of the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "68758f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page3 = requests.get('https://www.imdb.com/list/ls561664100/?sort=user_rating,desc&st_dt=&mode=detail&page=1')\n",
    "page3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be1b01e",
   "metadata": {},
   "source": [
    "To get the page content using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "fad07671",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup3 = BeautifulSoup(page3.content)\n",
    "#soup3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52f952",
   "metadata": {},
   "source": [
    "To scrap the required data of the webpage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6f98f013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 Idiots', 'Taare Zameen Par', 'Sardar Udham', 'Black Friday', 'Dangal', 'Khosla Ka Ghosla!', 'Drishyam', 'Tumbbad', 'Andhadhun', 'Gangs of Wasseypur', 'Zindagi Na Milegi Dobara', 'Swades: We, the People', 'Bhaag Milkha Bhaag', 'Shahid', 'Paan Singh Tomar', '1971', 'A Wednesday', 'PK', 'Article 15', 'Bajrangi Bhaijaan', 'Lagaan: Once Upon a Time in India', 'Pink', 'Dil Chahta Hai', 'Talvar', 'Munna Bhai M.B.B.S.', 'Kahaani', 'Chak De! India', 'Queen', 'Rang De Basanti', 'Barfi!', 'Masaan', 'Omkara', 'Hera Pheri', 'Black', 'Udaan', 'Iqbal', 'Haider', 'Maqbool', 'Ship of Theseus', 'Mukkabaaz', 'Gulaal', 'Company', 'Dor', 'Kadvi Hawa', 'My Name Is Khan', 'Kal Ho Naa Ho', 'Gully Boy', 'Jab We Met', 'Dev.D', 'Ugly', 'Johnny Gaddaar', 'Ankhon Dekhi', 'Sonchiriya', 'Hazaaron Khwaishein Aisi', 'The Lunchbox', 'Veer-Zaara', 'Kai po che!', 'English Vinglish', 'Ab Tak Chhappan', 'Stanley Ka Dabba', 'Dasvidaniya', 'Aligarh', 'Rockstar', 'Raazi', 'Sir', 'Kapoor & Sons', 'Udta Punjab', 'Water', 'Raincoat', 'Oye Lucky! Lucky Oye!', 'Tu Hai Mera Sunday', 'Wake Up Sid', 'Highway', 'The Sky Is Pink', 'Piku', 'Newton', 'Chandni Bar', 'Devdas', 'Delhi Belly', 'Trapped', 'October', 'Parched', 'Titli', 'Kabhi Khushi Kabhie Gham...', 'Jhund', 'A Death in the Gunj', 'Lootera', 'Peepli [Live]', 'No Smoking', 'Monsoon Wedding', 'Manto', 'Firaaq', 'LSD: Love, Sex Aur Dhokha', 'Eeb Allay Ooo!', 'Gangubai Kathiawadi', 'Badhaai Do', 'Thappad', 'RK/RKAY', 'Mukti Bhawan', 'Om Shanti Om']\n"
     ]
    }
   ],
   "source": [
    "# scrapping the names of the top 100 indian movies \n",
    "\n",
    "in_names=[] # empty list to store the names\n",
    "\n",
    "# loop for getting the multiple names \n",
    "\n",
    "for i in soup3.find_all('h3',class_= 'lister-item-header'):\n",
    "     in_names.append(i.text.split(\"\\n\")[2])\n",
    "        \n",
    "print(in_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "92c382fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8.4', '8.4', '8.4', '8.4', '8.3', '8.3', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.2', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8.1', '8', '8', '8', '8', '8', '8', '8', '8', '7.9', '7.9', '7.9', '7.9', '7.9', '7.9', '7.9', '7.9', '7.9', '7.9', '7.8', '7.8', '7.8', '7.8', '7.8', '7.8', '7.8', '7.8', '7.7', '7.7', '7.7', '7.7', '7.7', '7.7', '7.7', '7.7', '7.7', '7.6', '7.6', '7.6', '7.6', '7.6', '7.6', '7.5', '7.5', '7.5', '7.5', '7.5', '7.5', '7.4', '7.4', '7.4', '7.4', '7.4', '7.3', '7.3', '7.3', '7.3', '7.2', '7.2', '7', '7', '7', '7', '7', '6.7']\n"
     ]
    }
   ],
   "source": [
    "# scrapping the ratings of the top 100 indian movies \n",
    "\n",
    "in_rating=[] # empty list to store the ratings\n",
    "\n",
    "# loop for getting the multiple ratings\n",
    "\n",
    "for i in soup3.find_all('div',class_= 'ipl-rating-star small'):\n",
    "     in_rating.append(i.text.split(\"\\n\")[8])\n",
    "        \n",
    "print(in_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "90b8895e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(2009)', '(2007)', '(2021)', '(2004)', '(2016)', '(2006)', '(2015)', '(2018)', '(2018)', '(2012)', '(2011)', '(2004)', '(2013)', '(2012)', '(2012)', '(2007)', '(2008)', '(2014)', '(2019)', '(2015)', '(2001)', '(III) (2016)', '(2001)', '(2015)', '(2003)', '(2012)', '(2007)', '(2013)', '(2006)', '(2012)', '(2015)', '(2006)', '(2000)', '(2005)', '(2010)', '(2005)', '(2014)', '(2003)', '(2012)', '(2017)', '(2009)', '(2002)', '(2006)', '(2017)', '(2010)', '(2003)', '(2019)', '(2007)', '(2009)', '(2013)', '(2007)', '(2013)', '(2019)', '(2003)', '(2013)', '(2004)', '(2013)', '(2012)', '(2004)', '(2011)', '(2008)', '(2015)', '(2011)', '(2018)', '(I) (2018)', '(2016)', '(2016)', '(I) (2005)', '(2004)', '(2008)', '(2016)', '(2009)', '(I) (2014)', '(2019)', '(2015)', '(2017)', '(2001)', '(I) (2002)', '(2011)', '(XVII) (2016)', '(II) (2018)', '(2015)', '(2014)', '(2001)', '(2022)', '(2016)', '(I) (2013)', '(2010)', '(2007)', '(2001)', '(2018)', '(2008)', '(2010)', '(2019)', '(2022)', '(2022)', '(2020)', '(2021)', '(2016)', '(2007)']\n"
     ]
    }
   ],
   "source": [
    "# scrapping the year of release of the top 100 indian movies \n",
    "\n",
    "in_release_yr=[] # empty list to store the release year\n",
    "\n",
    "for i in soup3.find_all('span',class_= 'lister-item-year text-muted unbold'):\n",
    "     in_release_yr.append(i.text)\n",
    "        \n",
    "print(in_release_yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "21bc8ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Idiots   (2009)   8.4\n",
      "Taare Zameen Par   (2007)   8.4\n",
      "Sardar Udham   (2021)   8.4\n",
      "Black Friday   (2004)   8.4\n",
      "Dangal   (2016)   8.3\n",
      "Khosla Ka Ghosla!   (2006)   8.3\n",
      "Drishyam   (2015)   8.2\n",
      "Tumbbad   (2018)   8.2\n",
      "Andhadhun   (2018)   8.2\n",
      "Gangs of Wasseypur   (2012)   8.2\n",
      "Zindagi Na Milegi Dobara   (2011)   8.2\n",
      "Swades: We, the People   (2004)   8.2\n",
      "Bhaag Milkha Bhaag   (2013)   8.2\n",
      "Shahid   (2012)   8.2\n",
      "Paan Singh Tomar   (2012)   8.2\n",
      "1971   (2007)   8.2\n",
      "A Wednesday   (2008)   8.1\n",
      "PK   (2014)   8.1\n",
      "Article 15   (2019)   8.1\n",
      "Bajrangi Bhaijaan   (2015)   8.1\n",
      "Lagaan: Once Upon a Time in India   (2001)   8.1\n",
      "Pink   (III) (2016)   8.1\n",
      "Dil Chahta Hai   (2001)   8.1\n",
      "Talvar   (2015)   8.1\n",
      "Munna Bhai M.B.B.S.   (2003)   8.1\n",
      "Kahaani   (2012)   8.1\n",
      "Chak De! India   (2007)   8.1\n",
      "Queen   (2013)   8.1\n",
      "Rang De Basanti   (2006)   8.1\n",
      "Barfi!   (2012)   8.1\n",
      "Masaan   (2015)   8.1\n",
      "Omkara   (2006)   8.1\n",
      "Hera Pheri   (2000)   8.1\n",
      "Black   (2005)   8.1\n",
      "Udaan   (2010)   8.1\n",
      "Iqbal   (2005)   8.1\n",
      "Haider   (2014)   8\n",
      "Maqbool   (2003)   8\n",
      "Ship of Theseus   (2012)   8\n",
      "Mukkabaaz   (2017)   8\n",
      "Gulaal   (2009)   8\n",
      "Company   (2002)   8\n",
      "Dor   (2006)   8\n",
      "Kadvi Hawa   (2017)   8\n",
      "My Name Is Khan   (2010)   7.9\n",
      "Kal Ho Naa Ho   (2003)   7.9\n",
      "Gully Boy   (2019)   7.9\n",
      "Jab We Met   (2007)   7.9\n",
      "Dev.D   (2009)   7.9\n",
      "Ugly   (2013)   7.9\n",
      "Johnny Gaddaar   (2007)   7.9\n",
      "Ankhon Dekhi   (2013)   7.9\n",
      "Sonchiriya   (2019)   7.9\n",
      "Hazaaron Khwaishein Aisi   (2003)   7.9\n",
      "The Lunchbox   (2013)   7.8\n",
      "Veer-Zaara   (2004)   7.8\n",
      "Kai po che!   (2013)   7.8\n",
      "English Vinglish   (2012)   7.8\n",
      "Ab Tak Chhappan   (2004)   7.8\n",
      "Stanley Ka Dabba   (2011)   7.8\n",
      "Dasvidaniya   (2008)   7.8\n",
      "Aligarh   (2015)   7.8\n",
      "Rockstar   (2011)   7.7\n",
      "Raazi   (2018)   7.7\n",
      "Sir   (I) (2018)   7.7\n",
      "Kapoor & Sons   (2016)   7.7\n",
      "Udta Punjab   (2016)   7.7\n",
      "Water   (I) (2005)   7.7\n",
      "Raincoat   (2004)   7.7\n",
      "Oye Lucky! Lucky Oye!   (2008)   7.7\n",
      "Tu Hai Mera Sunday   (2016)   7.7\n",
      "Wake Up Sid   (2009)   7.6\n",
      "Highway   (I) (2014)   7.6\n",
      "The Sky Is Pink   (2019)   7.6\n",
      "Piku   (2015)   7.6\n",
      "Newton   (2017)   7.6\n",
      "Chandni Bar   (2001)   7.6\n",
      "Devdas   (I) (2002)   7.5\n",
      "Delhi Belly   (2011)   7.5\n",
      "Trapped   (XVII) (2016)   7.5\n",
      "October   (II) (2018)   7.5\n",
      "Parched   (2015)   7.5\n",
      "Titli   (2014)   7.5\n",
      "Kabhi Khushi Kabhie Gham...   (2001)   7.4\n",
      "Jhund   (2022)   7.4\n",
      "A Death in the Gunj   (2016)   7.4\n",
      "Lootera   (I) (2013)   7.4\n",
      "Peepli [Live]   (2010)   7.4\n",
      "No Smoking   (2007)   7.3\n",
      "Monsoon Wedding   (2001)   7.3\n",
      "Manto   (2018)   7.3\n",
      "Firaaq   (2008)   7.3\n",
      "LSD: Love, Sex Aur Dhokha   (2010)   7.2\n",
      "Eeb Allay Ooo!   (2019)   7.2\n",
      "Gangubai Kathiawadi   (2022)   7\n",
      "Badhaai Do   (2022)   7\n",
      "Thappad   (2020)   7\n",
      "RK/RKAY   (2021)   7\n",
      "Mukti Bhawan   (2016)   7\n",
      "Om Shanti Om   (2007)   6.7\n"
     ]
    }
   ],
   "source": [
    "for j in range(100):\n",
    "    print(in_names[j],\" \", in_release_yr[j], \" \",in_rating[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25877ca",
   "metadata": {},
   "source": [
    "TO MAKE THE DATAFRAME OF THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "85c6b94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>RATING</th>\n",
       "      <th>YEAR OF RELEASE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 Idiots</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(2009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Taare Zameen Par</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(2007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sardar Udham</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(2021)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Black Friday</td>\n",
       "      <td>8.4</td>\n",
       "      <td>(2004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dangal</td>\n",
       "      <td>8.3</td>\n",
       "      <td>(2016)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Badhaai Do</td>\n",
       "      <td>7</td>\n",
       "      <td>(2022)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Thappad</td>\n",
       "      <td>7</td>\n",
       "      <td>(2020)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>RK/RKAY</td>\n",
       "      <td>7</td>\n",
       "      <td>(2021)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Mukti Bhawan</td>\n",
       "      <td>7</td>\n",
       "      <td>(2016)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Om Shanti Om</td>\n",
       "      <td>6.7</td>\n",
       "      <td>(2007)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                NAME RATING YEAR OF RELEASE\n",
       "0           3 Idiots    8.4          (2009)\n",
       "1   Taare Zameen Par    8.4          (2007)\n",
       "2       Sardar Udham    8.4          (2021)\n",
       "3       Black Friday    8.4          (2004)\n",
       "4             Dangal    8.3          (2016)\n",
       "..               ...    ...             ...\n",
       "95        Badhaai Do      7          (2022)\n",
       "96           Thappad      7          (2020)\n",
       "97           RK/RKAY      7          (2021)\n",
       "98      Mukti Bhawan      7          (2016)\n",
       "99      Om Shanti Om    6.7          (2007)\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_df = pd.DataFrame( {\"NAME\": in_names, \"RATING\": in_rating, \"YEAR OF RELEASE\": in_release_yr } )\n",
    "in_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5aca503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING IN_DF TO A CSV FILE \n",
    "\n",
    "in_df.to_csv(\"imdb indian dataframe.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2fa5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001239e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c64daf71",
   "metadata": {},
   "source": [
    "# Question-4: Write s python program to display list of respected former presidents of India(i.e. Name , Term of office) from https://presidentofindia.nic.in/former-presidents.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0342e8",
   "metadata": {},
   "source": [
    "To get the source code of the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8441c1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page4 = requests.get(\"https://presidentofindia.nic.in/former-presidents.htm\")\n",
    "page4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a09707",
   "metadata": {},
   "source": [
    "To get the page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "b80f4c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup4 = BeautifulSoup(page4.content)\n",
    "#soup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adfe05c",
   "metadata": {},
   "source": [
    "To scrape the Name and Term of the Former Presidents  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ffe6437e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : Shri Ram Nath Kovind (birth - 1945) \n",
      "  Term of Office: 25 July, 2017 to 25 July, 2022 \n",
      "2 : Shri Pranab Mukherjee (1935-2020) \n",
      "  Term of Office: 25 July, 2012 to 25 July, 2017 \n",
      "3 : Smt Pratibha Devisingh Patil (birth - 1934) \n",
      "  Term of Office: 25 July, 2007 to 25 July, 2012 \n",
      "4 : DR. A.P.J. Abdul Kalam (1931-2015) \n",
      "  Term of Office: 25 July, 2002 to 25 July, 2007 \n",
      "5 : Shri K. R. Narayanan (1920 - 2005) \n",
      "  Term of Office: 25 July, 1997 to 25 July, 2002 \n",
      "6 : Dr Shankar Dayal Sharma (1918-1999) \n",
      "  Term of Office: 25 July, 1992 to 25 July, 1997 \n",
      "7 : Shri R Venkataraman (1910-2009) \n",
      "  Term of Office: 25 July, 1987 to 25 July, 1992 \n",
      "8 : Giani Zail Singh (1916-1994) \n",
      "  Term of Office: 25 July, 1982 to 25 July, 1987 \n",
      "9 : Shri Neelam Sanjiva Reddy (1913-1996) \n",
      "  Term of Office: 25 July, 1977 to 25 July, 1982 \n",
      "10 : Dr. Fakhruddin Ali Ahmed (1905-1977) \n",
      "  Term of Office: 24 August, 1974 to 11 February, 1977\n",
      "11 : Shri Varahagiri Venkata Giri (1894-1980) \n",
      "  Term of Office: 3 May, 1969 to 20 July, 1969 and 24 August, 1969 to 24 August, 1974\n",
      "12 : Dr. Zakir Husain (1897-1969) \n",
      "  Term of Office: 13 May, 1967 to 3 May, 1969\n",
      "13 : Dr. Sarvepalli Radhakrishnan (1888-1975) \n",
      "  Term of Office: 13 May, 1962 to 13 May, 1967\n",
      "14 : Dr. Rajendra Prasad (1884-1963)  \n",
      "  Term of Office: 26 January, 1950 to 13 May, 1962\n"
     ]
    }
   ],
   "source": [
    "pres_names=[] # empty list for the names \n",
    "pres_term=[] # empty list for the term of office\n",
    "\n",
    "# loop for scraping the name and term \n",
    "\n",
    "for i in soup4.find_all('div', class_=\"presidentListing\"):\n",
    "    pres_names.append(i.text.split(\"\\n\")[1])\n",
    "    pres_term.append(i.text.split(\"\\n\")[2])\n",
    "    \n",
    "# loop for printing the data in an organised manner\n",
    "\n",
    "for j in range(0,14):\n",
    "    print(j+1,\":\",pres_names[j] ,\"\\n \",pres_term[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "84f9d0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 25 July, 2017 to 25 July, 2022 '"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writing without Term of Office\n",
    "pres_term[0].split(\":\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe450a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6deed1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "282b8cbe",
   "metadata": {},
   "source": [
    "# Question-5:Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "\n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "\n",
    "b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "\n",
    "c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9642e2",
   "metadata": {},
   "source": [
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "584c271a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To get the source code of the webpage\n",
    "page5_1 = requests.get(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")\n",
    "page5_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "02fa7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the page content\n",
    "soup5_1 = BeautifulSoup(page5_1.content)\n",
    "#soup5_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d1862",
   "metadata": {},
   "source": [
    "TO SCRAP THE DATA OF TOP 10 ODI MEN'S TEAM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "310848dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   New Zealand    23   2,670   116\n",
      "2   England    30   3,400   113\n",
      "3   Australia    32   3,572   112\n",
      "4   India    35   3,866   110\n",
      "5   Pakistan    22   2,354   107\n",
      "6   South Africa    24   2,392   100\n",
      "7   Bangladesh    30   2,753   92\n",
      "8   Sri Lanka    30   2,677   89\n",
      "9   Afghanistan    19   1,380   73\n",
      "10   West Indies    41   2,902   71\n"
     ]
    }
   ],
   "source": [
    "team_o=[]  # EMPTY LIST TO STORE TEAM NAMES\n",
    "match_o=[] # EMPTY LIST TO STORE MATCHES\n",
    "point_o=[]  # EMPTY LIST TO STORE POINTS \n",
    "rating_o=[] # EMPTY LIST TO STORE RATINGS\n",
    "\n",
    "# CLASS FOR THE FIRST TEAM IS DIFFERENT FROM OTHERS SO TO SCRAP THE DATA OF FIRST TEAM SEPARATELY \n",
    "\n",
    "first_team = soup5_1.find('tr',class_=\"rankings-block__banner\")\n",
    "\n",
    "team_o.insert(0,first_team.text.split(\"\\n\")[4])   # to add first team name\n",
    "match_o.insert(0,first_team.text.split(\"\\n\")[7])  # to add first team's matches\n",
    "point_o.insert(0,first_team.text.split(\"\\n\")[8])  # to add first team's points\n",
    "rating_o.insert(0,first_team.text.split(\"\\n\")[10].split(\" \")[28]) # to add first team's rating\n",
    "\n",
    "# loop for adding the details of other teams \n",
    "\n",
    "for j in soup5_1.find_all('tr',class_=\"table-body\",limit=9):\n",
    "     \n",
    "    team_o.append(j.text.split(\"\\n\")[4])\n",
    "    match_o.append(j.text.split(\"\\n\")[7])\n",
    "    point_o.append(j.text.split(\"\\n\")[8])\n",
    "    rating_o.append(j.text.split(\"\\n\")[9])\n",
    "    \n",
    "# loop for displaying the desired data \n",
    " \n",
    "for j in range(10):\n",
    "    print(j+1,\" \", team_o[j],\"  \",match_o[j],\" \",point_o[j],\" \",rating_o[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "0e98149e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEAMS</th>\n",
       "      <th>MATCHES</th>\n",
       "      <th>POINTS</th>\n",
       "      <th>RATING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>23</td>\n",
       "      <td>2,670</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>England</td>\n",
       "      <td>30</td>\n",
       "      <td>3,400</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Australia</td>\n",
       "      <td>32</td>\n",
       "      <td>3,572</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>35</td>\n",
       "      <td>3,866</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>22</td>\n",
       "      <td>2,354</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>24</td>\n",
       "      <td>2,392</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>30</td>\n",
       "      <td>2,753</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>30</td>\n",
       "      <td>2,677</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>19</td>\n",
       "      <td>1,380</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>41</td>\n",
       "      <td>2,902</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          TEAMS MATCHES POINTS RATING\n",
       "0   New Zealand      23  2,670    116\n",
       "1       England      30  3,400    113\n",
       "2     Australia      32  3,572    112\n",
       "3         India      35  3,866    110\n",
       "4      Pakistan      22  2,354    107\n",
       "5  South Africa      24  2,392    100\n",
       "6    Bangladesh      30  2,753     92\n",
       "7     Sri Lanka      30  2,677     89\n",
       "8   Afghanistan      19  1,380     73\n",
       "9   West Indies      41  2,902     71"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA IN THE FORM OF DATAFRAME \n",
    "df_mo = pd.DataFrame( {\"TEAMS\":team_o,\"MATCHES\": match_o,\"POINTS\": point_o,\"RATING\":rating_o} )\n",
    "df_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219bb5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a83b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6720bdec",
   "metadata": {},
   "source": [
    " b) Top 10 ODI Batsmen along with the records of their team and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "732c3ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the source code of the webpage\n",
    "page5_2 = requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\")\n",
    "page5_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "7cf176be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the page content \n",
    "soup5_2 = BeautifulSoup(page5_2.content)\n",
    "# soup5_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29462e6e",
   "metadata": {},
   "source": [
    "To scrap the data of top 10 ODI Btasman "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "24440332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   Babar Azam    PAK   890\n",
      "2   Imam-ul-Haq    PAK   779\n",
      "3   Rassie van der Dussen    SA   766\n",
      "4   Quinton de Kock    SA   759\n",
      "5   David Warner    AUS   747\n",
      "6   Virat Kohli    IND   722\n",
      "7   Steve Smith    AUS   719\n",
      "8   Rohit Sharma    IND   718\n",
      "9   Jonny Bairstow    ENG   710\n",
      "10   Ross Taylor    NZ   701\n"
     ]
    }
   ],
   "source": [
    "bt_name=[]\n",
    "bt_team=[]\n",
    "bt_rating=[]\n",
    "pos_b=[]\n",
    "\n",
    "# CLASS FOR THE FIRST BATSMAN IS DIFFERENT FROM OTHERS SO TO SCRAP THE DATA OF FIRST BATSMAN SEPARATELY \n",
    "\n",
    "bt1 = soup5_2.find('tr',class_= \"rankings-block__banner\")\n",
    "bt_name.insert(0,(bt1.text.split(\"\\n\")[20]))\n",
    "bt_team.insert(0,(bt1.text.split(\"\\n\")[27]))\n",
    "bt_rating.insert(0,(bt1.text.split(\"\\n\")[31]))\n",
    "\n",
    "# loop for adding the name of other players  \n",
    "\n",
    "for j in soup5_2.find_all('td',class_=\"table-body__cell rankings-table__name name\",limit=9):\n",
    "    bt_name.append(j.text.split(\"\\n\")[1])\n",
    "    \n",
    "# loop for adding the name of other players  \n",
    "\n",
    "for j in soup5_2.find_all('span',class_=\"table-body__logo-text\",limit=9):\n",
    "    bt_team.append(j.text)\n",
    "    \n",
    "# loop for adding the rating of other players \n",
    "\n",
    "for j in soup5_2.find_all('td',class_=\"table-body__cell rating\",limit=9):\n",
    "    bt_rating.append(j.text)\n",
    "    \n",
    "# loop for displaying the desired data \n",
    "\n",
    "for j in range(10):\n",
    "    pos_b.insert(j,j+1) # to store the position of the batsmen \n",
    "    print(pos_b[j],\" \", bt_name[j],\"  \",bt_team[j],\" \",bt_rating[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "6ef053f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " POSITION                PLAYER TEAMS  RATING\n",
      "        1            Babar Azam    PAK    890\n",
      "        2           Imam-ul-Haq    PAK    779\n",
      "        3 Rassie van der Dussen     SA    766\n",
      "        4       Quinton de Kock     SA    759\n",
      "        5          David Warner    AUS    747\n",
      "        6           Virat Kohli    IND    722\n",
      "        7           Steve Smith    AUS    719\n",
      "        8          Rohit Sharma    IND    718\n",
      "        9        Jonny Bairstow    ENG    710\n",
      "       10           Ross Taylor     NZ    701\n"
     ]
    }
   ],
   "source": [
    "# DATA IN THE FORM OF DATAFRAME WITHOUT INDEX VALUE \n",
    "df_b = pd.DataFrame( {\"POSITION\":pos_b,\"PLAYER\": bt_name,\"TEAMS \": bt_team, \"RATING\":bt_rating} )\n",
    "print(df_b.to_string(index=False)) #printing of dataframe without index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52bbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb1dbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da468c6b",
   "metadata": {},
   "source": [
    " Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "fe482f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the source code of the webpage \n",
    "page5_3 = requests.get(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\")\n",
    "page5_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "f9edd1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the page content \n",
    "soup5_3 = BeautifulSoup(page5_3.content)\n",
    "# soup5_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81899ef4",
   "metadata": {},
   "source": [
    "To scrap the data of top 10 ODI BOWLERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "23b3377c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   Trent Boult    NZ   775\n",
      "2   Josh Hazlewood    AUS   727\n",
      "3   Mujeeb Ur Rahman    AFG   676\n",
      "4   Mitchell Starc    AUS   665\n",
      "5   Shaheen Afridi    PAK   661\n",
      "6   Mohammad Nabi    AFG   657\n",
      "7   Adam Zampa    AUS   655\n",
      "8   Mehedi Hasan    BAN   655\n",
      "9   Matt Henry    NZ   654\n",
      "10   Rashid Khan    AFG   651\n"
     ]
    }
   ],
   "source": [
    "bo_name=[]\n",
    "bo_team=[]\n",
    "bo_rating=[]\n",
    "pos_bo=[]\n",
    "\n",
    "# CLASS FOR THE FIRST BOWLER IS DIFFERENT FROM OTHERS SO TO SCRAP THE DATA OF FIRST BOWLER SEPARATELY \n",
    "\n",
    "bo1 = soup5_3.find('tr',class_= \"rankings-block__banner\")\n",
    "\n",
    "bo_name.insert(0,(bo1.text.split(\"\\n\")[20]))\n",
    "bo_team.insert(0,(bo1.text.split(\"\\n\")[27]))\n",
    "bo_rating.insert(0,(bo1.text.split(\"\\n\")[31]))\n",
    "\n",
    "# loop for adding the name of other players  \n",
    "\n",
    "for j in soup5_3.find_all('td',class_=\"table-body__cell rankings-table__name name\",limit=9):\n",
    "    bo_name.append(j.text.split(\"\\n\")[1])\n",
    "    \n",
    "# loop for adding the name of other players  \n",
    "\n",
    "for j in soup5_3.find_all('span',class_=\"table-body__logo-text\",limit=9):\n",
    "    bo_team.append(j.text)\n",
    "    \n",
    "# loop for adding the rating of other players \n",
    "\n",
    "for j in soup5_3.find_all('td',class_=\"table-body__cell rating\",limit=9):\n",
    "    bo_rating.append(j.text)   \n",
    "    \n",
    "# loop for displaying the desired data \n",
    "\n",
    "for j in range(10):\n",
    "    pos_bo.insert(j,j+1) # to store the position of the bowlers\n",
    "    print(pos_bo[j],\" \", bo_name[j],\"  \",bo_team[j],\" \",bo_rating[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "aaa33bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSITION</th>\n",
       "      <th>PLAYER</th>\n",
       "      <th>TEAMS</th>\n",
       "      <th>RATING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Mitchell Starc</td>\n",
       "      <td>AUS</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Shaheen Afridi</td>\n",
       "      <td>PAK</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Mohammad Nabi</td>\n",
       "      <td>AFG</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Adam Zampa</td>\n",
       "      <td>AUS</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Mehedi Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Matt Henry</td>\n",
       "      <td>NZ</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Rashid Khan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   POSITION            PLAYER TEAMS  RATING\n",
       "0         1       Trent Boult     NZ    775\n",
       "1         2    Josh Hazlewood    AUS    727\n",
       "2         3  Mujeeb Ur Rahman    AFG    676\n",
       "3         4    Mitchell Starc    AUS    665\n",
       "4         5    Shaheen Afridi    PAK    661\n",
       "5         6     Mohammad Nabi    AFG    657\n",
       "6         7        Adam Zampa    AUS    655\n",
       "7         8      Mehedi Hasan    BAN    655\n",
       "8         9        Matt Henry     NZ    654\n",
       "9        10       Rashid Khan    AFG    651"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA IN THE FORM OF DATAFRAME WITH INDEX \n",
    "df_bo = pd.DataFrame( {\"POSITION\":pos_bo,\"PLAYER\": bo_name,\"TEAMS \": bo_team, \"RATING\":bo_rating} )\n",
    "df_bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365295b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae4a561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "017bdcd6",
   "metadata": {},
   "source": [
    "# Question-6:Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "6457ed0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To get the source code of the webpage\n",
    "page6_1 = requests.get(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")\n",
    "page6_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "9833fad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the page content\n",
    "soup6_1 = BeautifulSoup(page6_1.content)\n",
    "# soup6_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "c810d9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   Australia    18   3,061   170\n",
      "2   South Africa    26   3,098   119\n",
      "3   England    25   2,904   116\n",
      "4   India    27   2,820   104\n",
      "5   New Zealand    24   2,425   101\n",
      "6   West Indies    24   2,334   97\n",
      "7   Bangladesh    12   932   78\n",
      "8   Thailand    8   572   72\n",
      "9   Pakistan    24   1,519   63\n",
      "10   Sri Lanka    8   353   44\n"
     ]
    }
   ],
   "source": [
    "# TO SCRAP THE DATA OF TOP 10 ODI WOMEN'S TEAM\n",
    "\n",
    "pos_wo=[] # EMPTY LIST TO STORE THE POSITION OF THE TEAM\n",
    "team_wo=[]  # EMPTY LIST TO STORE TEAM NAMES\n",
    "match_wo=[] # EMPTY LIST TO STORE MATCHES\n",
    "point_wo=[]  # EMPTY LIST TO STORE POINTS \n",
    "rating_wo=[] # EMPTY LIST TO STORE RATINGS\n",
    "\n",
    "# CLASS FOR THE FIRST TEAM IS DIFFERENT FROM OTHERS SO TO SCRAP THE DATA OF FIRST TEAM SEPARATELY \n",
    "\n",
    "first_team = soup6_1.find('tr',class_=\"rankings-block__banner\")\n",
    "\n",
    "team_wo.insert(0,first_team.text.split(\"\\n\")[4])   # to add first team name\n",
    "match_wo.insert(0,first_team.text.split(\"\\n\")[7])  # to add first team's matches\n",
    "point_wo.insert(0,first_team.text.split(\"\\n\")[8])  # to add first team's points\n",
    "rating_wo.insert(0,first_team.text.split(\"\\n\")[10].split(\" \")[28]) # to add first team's rating\n",
    "\n",
    "# loop for adding the details of other teams \n",
    "\n",
    "for j in soup6_1.find_all('tr',class_=\"table-body\",limit=9):\n",
    "     \n",
    "    team_wo.append(j.text.split(\"\\n\")[4])\n",
    "    match_wo.append(j.text.split(\"\\n\")[7])\n",
    "    point_wo.append(j.text.split(\"\\n\")[8])\n",
    "    rating_wo.append(j.text.split(\"\\n\")[9])\n",
    "    \n",
    "# loop for displaying the desired data \n",
    " \n",
    "for j in range(10):\n",
    "    pos_wo.insert(j,j+1)\n",
    "    print(pos_wo[j],\" \", team_wo[j],\"  \",match_wo[j],\" \",point_wo[j],\" \",rating_wo[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "c9f56083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " POSITION        TEAMS MATCHES POINTS RATING\n",
      "        1    Australia      18  3,061    170\n",
      "        2 South Africa      26  3,098    119\n",
      "        3      England      25  2,904    116\n",
      "        4        India      27  2,820    104\n",
      "        5  New Zealand      24  2,425    101\n",
      "        6  West Indies      24  2,334     97\n",
      "        7   Bangladesh      12    932     78\n",
      "        8     Thailand       8    572     72\n",
      "        9     Pakistan      24  1,519     63\n",
      "       10    Sri Lanka       8    353     44\n"
     ]
    }
   ],
   "source": [
    "# DATA IN THE FORM OF DATAFRAME WITHOUT INDEX\n",
    "df_wo = pd.DataFrame( {\"POSITION\":pos_wo,\"TEAMS\":team_wo,\"MATCHES\": match_wo,\"POINTS\": point_wo,\"RATING\":rating_wo} )\n",
    "print(df_wo.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b786cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b45052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3ff8a65",
   "metadata": {},
   "source": [
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "c40cd387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To get the source code of the webpage\n",
    "page6_2 = requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\")\n",
    "page6_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "a06d28af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the page content\n",
    "soup6_2 = BeautifulSoup(page6_2.content)\n",
    "# soup6_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c5cf6",
   "metadata": {},
   "source": [
    "TO SCRAPE THE DATA OF TOP 10 WOMEN BATTING PLAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "053427e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   Alyssa Healy    AUS   785\n",
      "2   Beth Mooney    AUS   749\n",
      "3   Laura Wolvaardt    SA   732\n",
      "4   Natalie Sciver    ENG   725\n",
      "5   Harmanpreet Kaur    IND   716\n",
      "6   Smriti Mandhana    IND   714\n",
      "7   Meg Lanning    AUS   710\n",
      "8   Rachael Haynes    AUS   701\n",
      "9   Amy Satterthwaite    NZ   661\n",
      "10   Chamari Athapaththu    SL   655\n"
     ]
    }
   ],
   "source": [
    "wbt_name=[] \n",
    "wbt_team=[]\n",
    "wbt_rating=[]\n",
    "pos_wb=[]\n",
    "\n",
    "# CLASS FOR THE FIRST PLAYER IS DIFFERENT FROM OTHERS SO TO SCRAP THE DATA OF FIRST PLAYER SEPARATELY \n",
    "\n",
    "wbt1 = soup6_2.find('tr',class_= \"rankings-block__banner\")\n",
    "wbt_name.insert(0,(wbt1.text.split(\"\\n\")[20]))\n",
    "wbt_team.insert(0,(wbt1.text.split(\"\\n\")[27]))\n",
    "wbt_rating.insert(0,(wbt1.text.split(\"\\n\")[31]))\n",
    "\n",
    "# loop for adding the name of other players  \n",
    "\n",
    "for j in soup6_2.find_all('td',class_=\"table-body__cell rankings-table__name name\",limit=9):\n",
    "    wbt_name.append(j.text.split(\"\\n\")[1])\n",
    "    \n",
    "# loop for adding the name of other players  \n",
    "\n",
    "for j in soup6_2.find_all('span',class_=\"table-body__logo-text\",limit=9):\n",
    "    wbt_team.append(j.text)\n",
    "    \n",
    "# loop for adding the rating of other players \n",
    "\n",
    "for j in soup6_2.find_all('td',class_=\"table-body__cell rating\",limit=9):\n",
    "    wbt_rating.append(j.text)\n",
    "    \n",
    "# loop for displaying the desired data \n",
    "\n",
    "for j in range(10):\n",
    "    pos_wb.insert(j,j+1) # to store the position of the batsmen \n",
    "    print(pos_wb[j],\" \", wbt_name[j],\"  \",wbt_team[j],\" \",wbt_rating[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "05b47ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSITION</th>\n",
       "      <th>PLAYER</th>\n",
       "      <th>TEAMS</th>\n",
       "      <th>RATING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Harmanpreet Kaur</td>\n",
       "      <td>IND</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Rachael Haynes</td>\n",
       "      <td>AUS</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Chamari Athapaththu</td>\n",
       "      <td>SL</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   POSITION               PLAYER TEAMS  RATING\n",
       "0         1         Alyssa Healy    AUS    785\n",
       "1         2          Beth Mooney    AUS    749\n",
       "2         3      Laura Wolvaardt     SA    732\n",
       "3         4       Natalie Sciver    ENG    725\n",
       "4         5     Harmanpreet Kaur    IND    716\n",
       "5         6      Smriti Mandhana    IND    714\n",
       "6         7          Meg Lanning    AUS    710\n",
       "7         8       Rachael Haynes    AUS    701\n",
       "8         9    Amy Satterthwaite     NZ    661\n",
       "9        10  Chamari Athapaththu     SL    655"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA IN THE FORM OF DATAFRAME WITH INDEX \n",
    "df_wb = pd.DataFrame( {\"POSITION\":pos_wb,\"PLAYER\": wbt_name,\"TEAMS \": wbt_team, \"RATING\":wbt_rating} )\n",
    "df_wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004f7b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c3560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c0934db",
   "metadata": {},
   "source": [
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "32117a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To get the source code of the webpage\n",
    "page6_3 = requests.get(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\")\n",
    "page6_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "7cd2937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the page content\n",
    "soup6_3 = BeautifulSoup(page6_3.content)\n",
    "# soup6_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4201dd1",
   "metadata": {},
   "source": [
    "TO SCRAPE ALL THE DESIRED DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "9c946422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   Hayley Matthews    WI   380\n",
      "2   Ellyse Perry    AUS   374\n",
      "3   Natalie Sciver    ENG   357\n",
      "4   Amelia Kerr    NZ   356\n",
      "5   Marizanne Kapp    SA   349\n",
      "6   Deepti Sharma    IND   322\n",
      "7   Ashleigh Gardner    AUS   270\n",
      "8   Jess Jonassen    AUS   246\n",
      "9   Jhulan Goswami    IND   214\n",
      "10   Katherine Brunt    ENG   207\n"
     ]
    }
   ],
   "source": [
    "pos_wa=[]\n",
    "wa_name=[]\n",
    "wa_team=[]\n",
    "wa_rating=[]\n",
    "\n",
    "# CLASS FOR THE FIRST PLAYER IS DIFFERENT FROM OTHERS SO TO SCRAP THE DATA OF FIRST PLAYER SEPARATELY \n",
    "\n",
    "ar1 = soup6_3.find('tr',class_ = \"rankings-block__banner\")\n",
    "\n",
    "wa_name.insert(0,(ar1.text.split(\"\\n\")[20]))\n",
    "wa_team.insert(0,(ar1.text.split(\"\\n\")[27]))\n",
    "wa_rating.insert(0,(ar1.text.split(\"\\n\")[31]))\n",
    "\n",
    "# loop for adding the name of other players  \n",
    "\n",
    "for j in soup6_3.find_all('td',class_=\"table-body__cell rankings-table__name name\",limit=9):\n",
    "    wa_name.append(j.text.split(\"\\n\")[1])\n",
    "    \n",
    "# loop for adding the name of other players  \n",
    "\n",
    "for j in soup6_3.find_all('span',class_=\"table-body__logo-text\",limit=9):\n",
    "    wa_team.append(j.text)\n",
    "    \n",
    "# loop for adding the rating of other players \n",
    "\n",
    "for j in soup6_3.find_all('td',class_=\"table-body__cell rating\",limit=9):\n",
    "    wa_rating.append(j.text)   \n",
    "    \n",
    "# loop for displaying the desired data \n",
    "\n",
    "for j in range(10):\n",
    "    \n",
    "    pos_wa.insert(j,j+1) # to store the position of the players\n",
    "    \n",
    "    print(pos_wa[j],\" \", wa_name[j],\"  \",wa_team[j],\" \",wa_rating[j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "f63c2561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " POSITION           PLAYER TEAMS  RATING\n",
      "        1  Hayley Matthews     WI    380\n",
      "        2     Ellyse Perry    AUS    374\n",
      "        3   Natalie Sciver    ENG    357\n",
      "        4      Amelia Kerr     NZ    356\n",
      "        5   Marizanne Kapp     SA    349\n",
      "        6    Deepti Sharma    IND    322\n",
      "        7 Ashleigh Gardner    AUS    270\n",
      "        8    Jess Jonassen    AUS    246\n",
      "        9   Jhulan Goswami    IND    214\n",
      "       10  Katherine Brunt    ENG    207\n"
     ]
    }
   ],
   "source": [
    "# DATA IN THE FORM OF DATAFRAME WITHOUT INDEX \n",
    "df_wa = pd.DataFrame( {\"POSITION\":pos_wa,\"PLAYER\": wa_name,\"TEAMS \": wa_team, \"RATING\":wa_rating} )\n",
    "print(df_wa.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176d1b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4bca8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25bf7737",
   "metadata": {},
   "source": [
    "# Question-7: Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
    "i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035eaf54",
   "metadata": {},
   "source": [
    "To get the source code of the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "e12c7247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page7=requests.get(\"https://www.cnbc.com/world/?region=world\")\n",
    "page7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "208fa077",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To get the page content\n",
    "soup7 = BeautifulSoup(page7.content)\n",
    "# soup7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5f05cf",
   "metadata": {},
   "source": [
    "To scrape the data of the news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "fce89c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   Rolls-Royce uses hydrogen produced with wind and tidal power to test jet engine   22 Min Ago   https://www.cnbc.com/2022/12/02/rolls-royce-uses-green-hydrogen-in-jet-engine-test.html\n",
      "2   Singapore and New York are the most expensive cities to live in, EIU says   44 Min Ago   https://www.cnbc.com/2022/12/02/singapore-and-new-york-are-the-most-expensive-cities-to-live-in-eiu.html\n",
      "3   Hong Kong's Hang Seng still in bear market territory despite best month since 1998   49 Min Ago   https://www.cnbc.com/2022/12/02/hang-seng-still-in-bear-market-territory-despite-best-month-since-1998.html\n",
      "4   Tesla CEO Elon Musk kicks off first Semi truck deliveries   4 Hours Ago   https://www.cnbc.com/2022/12/01/tesla-ceo-elon-musk-kicks-off-semi-truck-deliveries.html\n",
      "5   Citi names 6 global stocks that capture both 'defensive growth and value'   6 Hours Ago   https://www.cnbc.com/2022/12/02/citi-names-6-stocks-that-capture-both-defensive-growth-and-value.html\n",
      "6   BlackRock unit tells investors it's time for a new portfolio playbook   6 Hours Ago   https://www.cnbc.com/2022/12/02/blackrocks-ishares-on-how-to-invest-in-an-environment-of-higher-rates.html\n",
      "7   Cramer's lightning round: Seagate is not for this market   7 Hours Ago   https://www.cnbc.com/2022/12/01/cramers-lightning-round-seagate-is-not-for-this-market.html\n",
      "8   Wrong Covid test results in China raise concerns 'the pandemic may never end'   7 Hours Ago   https://www.cnbc.com/2022/12/02/china-people-worry-wrong-covid-test-results-mean-pandemic-wont-end.html\n",
      "9   Jim Cramer says he likes these 3 restaurant stocks for a ‘normalizing’ economy   7 Hours Ago   https://www.cnbc.com/2022/12/01/jim-cramer-says-he-likes-these-3-restaurant-stocks.html\n",
      "10   Japanese stocks lead losses in Asia as investors seek clarity on China's Covid rule changes   7 Hours Ago   https://www.cnbc.com/2022/12/02/asia-pacific-markets-jobs.html\n"
     ]
    }
   ],
   "source": [
    "# creating emoty lists to store the data \n",
    "head=[]\n",
    "time=[]\n",
    "link=[]\n",
    "\n",
    "# loop for getting the news headlines \n",
    "for i in soup7.find_all('li',class_=\"LatestNews-item\",limit=10):\n",
    "    head.append(i.text.split(\"Ago\")[1])\n",
    "\n",
    "# loop for getting the time of the news \n",
    "for i in soup7.find_all('time',class_=\"LatestNews-timestamp\",limit=10):\n",
    "    time.append(i.text)\n",
    "    \n",
    "# loop for getting the link of the news    \n",
    "for i in soup7.find_all('a',class_=\"LatestNews-headline\",limit=10):\n",
    "    link.append(i.get('href'))\n",
    "\n",
    "# loop for displaying the data \n",
    "for j in range(10):\n",
    "    print(j+1,\" \",head[j],\" \",time[j],\" \",link[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "4f392e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HEADLINE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>LINK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rolls-Royce uses hydrogen produced with wind a...</td>\n",
       "      <td>22 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/rolls-royce-us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Singapore and New York are the most expensive ...</td>\n",
       "      <td>44 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/singapore-and-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hong Kong's Hang Seng still in bear market ter...</td>\n",
       "      <td>49 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/hang-seng-stil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tesla CEO Elon Musk kicks off first Semi truck...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/01/tesla-ceo-elon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Citi names 6 global stocks that capture both '...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/citi-names-6-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BlackRock unit tells investors it's time for a...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/blackrocks-ish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cramer's lightning round: Seagate is not for t...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/01/cramers-lightn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wrong Covid test results in China raise concer...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/china-people-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jim Cramer says he likes these 3 restaurant st...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/01/jim-cramer-say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Japanese stocks lead losses in Asia as investo...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/12/02/asia-pacific-m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            HEADLINE        TIME   \\\n",
       "0  Rolls-Royce uses hydrogen produced with wind a...   22 Min Ago   \n",
       "1  Singapore and New York are the most expensive ...   44 Min Ago   \n",
       "2  Hong Kong's Hang Seng still in bear market ter...   49 Min Ago   \n",
       "3  Tesla CEO Elon Musk kicks off first Semi truck...  4 Hours Ago   \n",
       "4  Citi names 6 global stocks that capture both '...  6 Hours Ago   \n",
       "5  BlackRock unit tells investors it's time for a...  6 Hours Ago   \n",
       "6  Cramer's lightning round: Seagate is not for t...  7 Hours Ago   \n",
       "7  Wrong Covid test results in China raise concer...  7 Hours Ago   \n",
       "8  Jim Cramer says he likes these 3 restaurant st...  7 Hours Ago   \n",
       "9  Japanese stocks lead losses in Asia as investo...  7 Hours Ago   \n",
       "\n",
       "                                                LINK  \n",
       "0  https://www.cnbc.com/2022/12/02/rolls-royce-us...  \n",
       "1  https://www.cnbc.com/2022/12/02/singapore-and-...  \n",
       "2  https://www.cnbc.com/2022/12/02/hang-seng-stil...  \n",
       "3  https://www.cnbc.com/2022/12/01/tesla-ceo-elon...  \n",
       "4  https://www.cnbc.com/2022/12/02/citi-names-6-s...  \n",
       "5  https://www.cnbc.com/2022/12/02/blackrocks-ish...  \n",
       "6  https://www.cnbc.com/2022/12/01/cramers-lightn...  \n",
       "7  https://www.cnbc.com/2022/12/02/china-people-w...  \n",
       "8  https://www.cnbc.com/2022/12/01/jim-cramer-say...  \n",
       "9  https://www.cnbc.com/2022/12/02/asia-pacific-m...  "
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA IN THE FORM OF DATAFRAME \n",
    "df7 = pd.DataFrame( {\"HEADLINE\": head, \"TIME \": time, \" LINK\": link} )\n",
    "df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa6543c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e573d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f0192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d640fd5a",
   "metadata": {},
   "source": [
    "# Question-8: Write a python program to scrape the details of most downloaded articles from AI in last 90 days. \n",
    "https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "\n",
    "Scrape below mentioned details :\n",
    "i) Paper Title \n",
    "ii) Authors\n",
    "iii) Published Date \n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "26d184e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the source code of the webpage\n",
    "page8 = requests.get(\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\")\n",
    "page8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "3dce8450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get the page content\n",
    "soup8 = BeautifulSoup(page8.content)\n",
    "# soup8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd8d86",
   "metadata": {},
   "source": [
    "To scrape the desired data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "cd85954b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Reward is enough   Silver, David, Singh, Satinder, Precup, Doina, Sutton, Richard S.    October 2021   https://www.sciencedirect.com/science/article/pii/S0004370221000862\n",
      "2 Making sense of raw input   Evans, Richard, Bošnjak, Matko and 5 more   October 2021   https://www.sciencedirect.com/science/article/pii/S0004370221000722\n",
      "3 Law and logic: A review from an argumentation perspective   Prakken, Henry, Sartor, Giovanni    October 2015   https://www.sciencedirect.com/science/article/pii/S0004370215000910\n",
      "4 Creativity and artificial intelligence   Boden, Margaret A.    August 1998   https://www.sciencedirect.com/science/article/pii/S0004370298000551\n",
      "5 Artificial cognition for social human–robot interaction: An implementation   Lemaignan, Séverin, Warnier, Mathieu and 3 more   June 2017   https://www.sciencedirect.com/science/article/pii/S0004370216300790\n",
      "6 Explanation in artificial intelligence: Insights from the social sciences   Miller, Tim    February 2019   https://www.sciencedirect.com/science/article/pii/S0004370218305988\n",
      "7 Making sense of sensory input   Evans, Richard, Hernández-Orallo, José and 3 more   April 2021   https://www.sciencedirect.com/science/article/pii/S0004370220301855\n",
      "8 Conflict-based search for optimal multi-agent pathfinding   Sharon, Guni, Stern, Roni, Felner, Ariel, Sturtevant, Nathan R.    February 2015   https://www.sciencedirect.com/science/article/pii/S0004370214001386\n",
      "9 Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning   Sutton, Richard S., Precup, Doina, Singh, Satinder    August 1999   https://www.sciencedirect.com/science/article/pii/S0004370299000521\n",
      "10 The Hanabi challenge: A new frontier for AI research   Bard, Nolan, Foerster, Jakob N. and 13 more   March 2020   https://www.sciencedirect.com/science/article/pii/S0004370219300116\n",
      "11 Evaluating XAI: A comparison of rule-based and example-based explanations   van der Waa, Jasper, Nieuwburg, Elisabeth, Cremers, Anita, Neerincx, Mark    February 2021   https://www.sciencedirect.com/science/article/pii/S0004370220301533\n",
      "12 Argumentation in artificial intelligence   Bench-Capon, T.J.M., Dunne, Paul E.    October 2007   https://www.sciencedirect.com/science/article/pii/S0004370207000793\n",
      "13 Algorithms for computing strategies in two-player simultaneous move games   Bošanský, Branislav, Lisý, Viliam and 3 more   August 2016   https://www.sciencedirect.com/science/article/pii/S0004370216300285\n",
      "14 Multiple object tracking: A literature review   Luo, Wenhan, Xing, Junliang and 4 more   April 2021   https://www.sciencedirect.com/science/article/pii/S0004370220301958\n",
      "15 Selection of relevant features and examples in machine learning   Blum, Avrim L., Langley, Pat    December 1997   https://www.sciencedirect.com/science/article/pii/S0004370297000635\n",
      "16 A survey of inverse reinforcement learning: Challenges, methods and progress   Arora, Saurabh, Doshi, Prashant    August 2021   https://www.sciencedirect.com/science/article/pii/S0004370221000515\n",
      "17 Explaining individual predictions when features are dependent: More accurate approximations to Shapley values   Aas, Kjersti, Jullum, Martin, Løland, Anders    September 2021   https://www.sciencedirect.com/science/article/pii/S0004370221000539\n",
      "18 A review of possible effects of cognitive biases on interpretation of rule-based machine learning models   Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Johannes    June 2021   https://www.sciencedirect.com/science/article/pii/S0004370221000096\n",
      "19 Integrating social power into the decision-making of cognitive agents   Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   https://www.sciencedirect.com/science/article/pii/S0004370216300868\n",
      "20 “That's (not) the output I expected!” On the role of end user expectations in creating explanations of AI systems   Riveiro, Maria, Thill, Serge    September 2021   https://www.sciencedirect.com/science/article/pii/S0004370221000588\n",
      "21 Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies   Kenny, Eoin M., Ford, Courtney, Quinn, Molly, Keane, Mark T.    May 2021   https://www.sciencedirect.com/science/article/pii/S0004370221000102\n",
      "22 Algorithm runtime prediction: Methods & evaluation   Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyton-Brown, Kevin    January 2014   https://www.sciencedirect.com/science/article/pii/S0004370213001082\n",
      "23 Wrappers for feature subset selection   Kohavi, Ron, John, George H.    December 1997   https://www.sciencedirect.com/science/article/pii/S000437029700043X\n",
      "24 Commonsense visual sensemaking for autonomous driving – On generalised neurosymbolic online abduction integrating vision and semantics   Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srikrishna    October 2021   https://www.sciencedirect.com/science/article/pii/S0004370221000734\n",
      "25 Quantum computation, quantum theory and AI   Ying, Mingsheng    February 2010   https://www.sciencedirect.com/science/article/pii/S0004370209001398\n"
     ]
    }
   ],
   "source": [
    "# creating empty lists to store all the data of the articles\n",
    "a_title=[]\n",
    "a_authors=[]\n",
    "a_date=[]\n",
    "a_url=[]\n",
    "\n",
    "# loop for the title of the articles\n",
    "\n",
    "for i in soup8.find_all('h2',class_=\"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\"):\n",
    "    a_title.append(i.text)\n",
    "\n",
    "# loop for the authors of the articles\n",
    "\n",
    "for i in soup8.find_all('span',class_=\"sc-1w3fpd7-0 dnCnAO\"):\n",
    "    a_authors.append(i.text)\n",
    "\n",
    "# loop for the Date of Publication of the articles\n",
    "\n",
    "for i in soup8.find_all('span',class_=\"sc-1thf9ly-2 dvggWt\"):\n",
    "    a_date.append(i.text)\n",
    "\n",
    "# loop for the URL of the articles\n",
    "\n",
    "for i in soup8.find_all('a',class_=\"sc-5smygv-0 fIXTHm\"):\n",
    "    a_url.append(i.get('href'))\n",
    "\n",
    "# loop for displaying the data of articles\n",
    "\n",
    "for j in range(25):\n",
    "    print(j+1,a_title[j],\" \",a_authors[j],\" \",a_date[j],\" \",a_url[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "a88e4205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>AUTHORS</th>\n",
       "      <th>PUBLISHED DATE</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TITLE  \\\n",
       "0                                    Reward is enough   \n",
       "1                           Making sense of raw input   \n",
       "2   Law and logic: A review from an argumentation ...   \n",
       "3              Creativity and artificial intelligence   \n",
       "4   Artificial cognition for social human–robot in...   \n",
       "5   Explanation in artificial intelligence: Insigh...   \n",
       "6                       Making sense of sensory input   \n",
       "7   Conflict-based search for optimal multi-agent ...   \n",
       "8   Between MDPs and semi-MDPs: A framework for te...   \n",
       "9   The Hanabi challenge: A new frontier for AI re...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11           Argumentation in artificial intelligence   \n",
       "12  Algorithms for computing strategies in two-pla...   \n",
       "13      Multiple object tracking: A literature review   \n",
       "14  Selection of relevant features and examples in...   \n",
       "15  A survey of inverse reinforcement learning: Ch...   \n",
       "16  Explaining individual predictions when feature...   \n",
       "17  A review of possible effects of cognitive bias...   \n",
       "18  Integrating social power into the decision-mak...   \n",
       "19  “That's (not) the output I expected!” On the r...   \n",
       "20  Explaining black-box classifiers using post-ho...   \n",
       "21  Algorithm runtime prediction: Methods & evalua...   \n",
       "22              Wrappers for feature subset selection   \n",
       "23  Commonsense visual sensemaking for autonomous ...   \n",
       "24         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                              AUTHORS  PUBLISHED DATE  \\\n",
       "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
       "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
       "2                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
       "3                                 Boden, Margaret A.      August 1998   \n",
       "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
       "5                                        Miller, Tim    February 2019   \n",
       "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
       "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
       "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
       "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
       "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
       "11               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
       "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
       "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
       "14                      Blum, Avrim L., Langley, Pat    December 1997   \n",
       "15                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
       "16      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
       "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
       "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
       "19                      Riveiro, Maria, Thill, Serge   September 2021   \n",
       "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
       "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
       "22                      Kohavi, Ron, John, George H.    December 1997   \n",
       "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
       "24                                   Ying, Mingsheng    February 2010   \n",
       "\n",
       "                                                  URL  \n",
       "0   https://www.sciencedirect.com/science/article/...  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA IN THE FORM OF DATAFRAME \n",
    "df8 = pd.DataFrame( {\"TITLE\": a_title, \"AUTHORS\": a_authors, \"PUBLISHED DATE\": a_date, \"URL\": a_url} )\n",
    "df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "e53983f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the csv file \n",
    "df8.to_csv(\"article.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce6a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad52ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d1cc9c1",
   "metadata": {},
   "source": [
    "# Question-9: ) Write a python program to scrape mentioned details from dineout.co.in :\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location \n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "010f8669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the source code of the webpage\n",
    "page9 = requests.get(\"https://www.dineout.co.in/delhi-restaurants/buffet-special\")\n",
    "page9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "801c47de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get the page content \n",
    "soup9 = BeautifulSoup(page9.content)\n",
    "# soup9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005b065",
   "metadata": {},
   "source": [
    "TO SCRAPE ALL THE DESIRED DATA OF THE RESTRAUNTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "b01d4b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   Castle Barbeque    Chinese, North Indian   Connaught Place, Central Delhi   4.1   https://im1.dineout.co.in/images/uploads/restaurant/sharpen/8/k/b/p86792-16062953735fbe1f4d3fb7e.jpg?tr=tr:n-medium\n",
      "2   Jungle Jamboree    North Indian, Asian, Italian   3CS Mall,Lajpat Nagar - 3, South Delhi   3.9   https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/p/m/p59633-166088382462ff137009010.jpg?tr=tr:n-medium\n",
      "3   Cafe Knosh    Italian, Continental   The Leela Ambience Convention Hotel,Shahdara, East Delhi   4.3   https://im1.dineout.co.in/images/uploads/restaurant/sharpen/4/p/m/p406-15438184745c04ccea491bc.jpg?tr=tr:n-medium\n",
      "4   Castle Barbeque    Chinese, North Indian   Pacific Mall,Tagore Garden, West Delhi   3.9   https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/j/o/p38113-15959192065f1fcb666130c.jpg?tr=tr:n-medium\n",
      "5   The Barbeque Company    North Indian, Chinese   Gardens Galleria,Sector 38A, Noida   4   https://im1.dineout.co.in/images/uploads/restaurant/sharpen/7/p/k/p79307-16051787755fad1597f2bf9.jpg?tr=tr:n-medium\n",
      "6   India Grill    North Indian, Italian   Hilton Garden Inn,Saket, South Delhi   3.9   https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/v/t/p2687-1482477169585cce712b90f.jpg?tr=tr:n-medium\n",
      "7   Delhi Barbeque    North Indian   Taurus Sarovar Portico,Mahipalpur, South Delhi   3.6   https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/d/i/p52501-1661855212630de5eceb6d2.jpg?tr=tr:n-medium\n",
      "8   The Monarch - Bar Be Que Village    North Indian   Indirapuram Habitat Centre,Indirapuram, Ghaziabad   3.8   https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/n/o/p34822-15599107305cfa594a13c24.jpg?tr=tr:n-medium\n",
      "9   Indian Grill Room    North Indian, Mughlai   Suncity Business Tower,Golf Course Road, Gurgaon   4.3   https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/y/f/p549-165000147262590640c0afc.jpg?tr=tr:n-medium\n"
     ]
    }
   ],
   "source": [
    "# creating empty lists to store the data \n",
    "\n",
    "res_name=[]\n",
    "res_cuis=[]\n",
    "res_loc=[]\n",
    "res_rating=[]\n",
    "res_iurl=[]\n",
    "\n",
    "# loop to get the restraunt names:\n",
    "\n",
    "for i in soup9.find_all('a',class_=\"restnt-name ellipsis\"):\n",
    "    res_name.append(i.text)\n",
    "\n",
    "# loop to get the restraunt cuisines:\n",
    "\n",
    "for i in soup9.find_all('span',class_=\"double-line-ellipsis\"):\n",
    "    res_cuis.append(i.text.split(\"|\")[1])\n",
    "    \n",
    "# loop to get the restraunt location:\n",
    "\n",
    "for i in soup9.find_all('div',class_=\"restnt-loc ellipsis\"):\n",
    "    res_loc.append(i.text)\n",
    "    \n",
    "# loop to get the restraunt rating:\n",
    "\n",
    "for i in soup9.find_all('div',class_=\"restnt-rating rating-4\"):\n",
    "    res_rating.append(i.text)\n",
    "    \n",
    "# loop to get the image url of thr restraunt:\n",
    "\n",
    "for i in soup9.find_all('img',class_=\"no-img\"):\n",
    "    res_iurl.append(i.get('data-src'))  # get is for fetching the url\n",
    "    \n",
    "# loop to display all the desired data \n",
    "\n",
    "for j in range(9):\n",
    "    print(j+1,\" \",res_name[j],\" \",res_cuis[j],\" \",res_loc[j],\" \",res_rating[j],\" \",res_iurl[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "aadc87b9",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>CUISINE</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>RATING</th>\n",
       "      <th>IMAGE URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree</td>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>North Indian, Mughlai</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               NAME                        CUISINE  \\\n",
       "0                   Castle Barbeque          Chinese, North Indian   \n",
       "1                   Jungle Jamboree   North Indian, Asian, Italian   \n",
       "2                        Cafe Knosh           Italian, Continental   \n",
       "3                   Castle Barbeque          Chinese, North Indian   \n",
       "4              The Barbeque Company          North Indian, Chinese   \n",
       "5                       India Grill          North Indian, Italian   \n",
       "6                    Delhi Barbeque                   North Indian   \n",
       "7  The Monarch - Bar Be Que Village                   North Indian   \n",
       "8                 Indian Grill Room          North Indian, Mughlai   \n",
       "\n",
       "                                            LOCATION RATING  \\\n",
       "0                     Connaught Place, Central Delhi    4.1   \n",
       "1             3CS Mall,Lajpat Nagar - 3, South Delhi    3.9   \n",
       "2  The Leela Ambience Convention Hotel,Shahdara, ...    4.3   \n",
       "3             Pacific Mall,Tagore Garden, West Delhi    3.9   \n",
       "4                 Gardens Galleria,Sector 38A, Noida      4   \n",
       "5               Hilton Garden Inn,Saket, South Delhi    3.9   \n",
       "6     Taurus Sarovar Portico,Mahipalpur, South Delhi    3.6   \n",
       "7  Indirapuram Habitat Centre,Indirapuram, Ghaziabad    3.8   \n",
       "8   Suncity Business Tower,Golf Course Road, Gurgaon    4.3   \n",
       "\n",
       "                                           IMAGE URL  \n",
       "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA IN DATAFRAME \n",
    "df9 = pd.DataFrame( {\"NAME\": res_name, \"CUISINE\":res_cuis, \"LOCATION\": res_loc, \"RATING\": res_rating, \"IMAGE URL\": res_iurl } )\n",
    "df9\n",
    "#df9.to_csv(\"dineout.csv\") in the form of csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffbc28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef29ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed703b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cda563b4",
   "metadata": {},
   "source": [
    "# Question-10: Write a python program to scrape the details of top publications from Google Scholar from \n",
    "https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "i) Rank \n",
    "ii) Publication\n",
    "iii) h5-index\n",
    " iv) h5-median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "3b7b9cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the source code of the webpage\n",
    "page10 = requests.get(\"https://scholar.google.com/citations?view_op=top_venues&hl=en\")\n",
    "page10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "5ef496b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the page content \n",
    "soup10 = BeautifulSoup(page10.content)\n",
    "# soup10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc7b9e",
   "metadata": {},
   "source": [
    "TO SCRAPE THE DESIRED DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "040b389e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.   Nature   444   667\n",
      "2.   The New England Journal of Medicine   432   780\n",
      "3.   Science   401   614\n",
      "4.   IEEE/CVF Conference on Computer Vision and Pattern Recognition   389   627\n",
      "5.   The Lancet   354   635\n",
      "6.   Advanced Materials   312   418\n",
      "7.   Nature Communications   307   428\n",
      "8.   Cell   300   505\n",
      "9.   International Conference on Learning Representations   286   533\n",
      "10.   Neural Information Processing Systems   278   436\n"
     ]
    }
   ],
   "source": [
    "# creating empty lists to store the data\n",
    "rank=[]\n",
    "pub=[]\n",
    "h5_in=[]\n",
    "h5_med=[]\n",
    "\n",
    "# loop to get the rank of all publications\n",
    "\n",
    "for i in soup10.find_all('td',class_=\"gsc_mvt_p\"):\n",
    "    rank.append(i.text)\n",
    "    \n",
    "# loop to get the Name of all publications\n",
    "\n",
    "for i in soup10.find_all('td',class_=\"gsc_mvt_t\"):\n",
    "    pub.append(i.text)\n",
    "    \n",
    "# loop to get H5-index of all publications\n",
    "\n",
    "for i in soup10.find_all('a',class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "    h5_in.append(i.text)\n",
    "    \n",
    "# loop to get H5-median of all publications\n",
    "\n",
    "for i in soup10.find_all('span',class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "    h5_med.append(i.text)\n",
    "\n",
    "# loop to display all the desired data \n",
    "\n",
    "for j in range(100):\n",
    "    print(rank[j],\" \",pub[j],\" \",h5_in[j],\" \",h5_med[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "a1516f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RANK</th>\n",
       "      <th>PUBLICATION</th>\n",
       "      <th>H5-INDEX</th>\n",
       "      <th>H5-MEDIAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Nature</td>\n",
       "      <td>444</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>432</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Science</td>\n",
       "      <td>401</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>389</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>354</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.</td>\n",
       "      <td>Journal of Business Research</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.</td>\n",
       "      <td>Molecular Cancer</td>\n",
       "      <td>145</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>145</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.</td>\n",
       "      <td>Nature Climate Change</td>\n",
       "      <td>144</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>144</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    RANK                                        PUBLICATION H5-INDEX H5-MEDIAN\n",
       "0     1.                                             Nature      444       667\n",
       "1     2.                The New England Journal of Medicine      432       780\n",
       "2     3.                                            Science      401       614\n",
       "3     4.  IEEE/CVF Conference on Computer Vision and Pat...      389       627\n",
       "4     5.                                         The Lancet      354       635\n",
       "..   ...                                                ...      ...       ...\n",
       "95   96.                       Journal of Business Research      145       233\n",
       "96   97.                                   Molecular Cancer      145       209\n",
       "97   98.                                            Sensors      145       201\n",
       "98   99.                              Nature Climate Change      144       228\n",
       "99  100.                    IEEE Internet of Things Journal      144       212\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA IN THE FORM OF DATAFRAME \n",
    "df10 = pd.DataFrame( {\"RANK\":rank, \"PUBLICATION\":pub, \"H5-INDEX\":h5_in, \"H5-MEDIAN\":h5_med} )\n",
    "df10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de252b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
